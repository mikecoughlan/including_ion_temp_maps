{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import utils\n",
    "from scipy.special import erf\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (auc, brier_score_loss, confusion_matrix,\n",
    "                             mean_absolute_error, mean_squared_error,\n",
    "                             precision_recall_curve, r2_score, roc_curve,\n",
    "\t\t\t\t\t\t\t precision_score, recall_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'swmag'\n",
    "shap_dir = 'outputs/shap_values'\n",
    "scaler_dir = 'outputs/scalers'\n",
    "results_dir = 'outputs/rsd'\n",
    "VERSION = 'swmag_v6-1'\n",
    "shap_files = glob.glob(f'{shap_dir}/*{VERSION}*.pkl')\n",
    "scaler_files = glob.glob(f'{scaler_dir}/*{VERSION}.pkl')\n",
    "results_files = glob.glob(f'{scaler_dir}/*{VERSION}.feather')\n",
    "with open('cluster_dict.pkl', 'rb') as f:\n",
    "\tcluster_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict['greenland_cluster']['regions']['GRL-0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for cluster in cluster_dict.values():\n",
    "\tfor key, region in cluster['regions'].items():\n",
    "\t\tresults[key] = {}\n",
    "\t\tresults[key]['swmag_results'] = pd.read_feather(f'{results_dir}/non_twins_modeling_region_{key}_version_swmag_v6-1.feather')\n",
    "\t\tresults[key]['maxpool_results'] = pd.read_feather(f'{results_dir}/twins_modeling_region_{key}_version_twins_v_maxpooling.feather')\n",
    "\t\tresults[key]['mean_lat'] = utils.getting_mean_lat(region['stations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swmag_auc, maxpool_auc, mean_lat = [], [], []\n",
    "for key in results.keys():\n",
    "\n",
    "\tprec, rec, __ = precision_recall_curve(y_true=results[key]['swmag_results']['actual'], probas_pred=results[key]['swmag_results']['predicted_mean'])\n",
    "\tswmag_auc.append(auc(rec, prec))\n",
    "\tprec, rec, __ = precision_recall_curve(y_true=results[key]['maxpool_results']['actual'], probas_pred=results[key]['maxpool_results']['predicted_mean'])\n",
    "\tmaxpool_auc.append(auc(rec, prec))\n",
    "\tmean_lat.append(results[key]['mean_lat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swmag_plot_df = pd.DataFrame({'mean_lat':mean_lat, 'swmag_auc':swmag_auc})\n",
    "maxpool_plot_df = pd.DataFrame({'mean_lat':mean_lat, 'maxpool_auc':maxpool_auc})\n",
    "\n",
    "swmag_plot_df.sort_values(by='mean_lat', inplace=True)\n",
    "maxpool_plot_df.sort_values(by='mean_lat', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1,figsize=(10,7))\n",
    "axes.scatter(swmag_plot_df['mean_lat'], swmag_plot_df['swmag_auc'], label='swmag', color='blue')\n",
    "axes.scatter(maxpool_plot_df['mean_lat'], maxpool_plot_df['maxpool_auc'], label='maxpool', color='orange')\n",
    "\n",
    "swmag_z = np.polyfit(swmag_plot_df['mean_lat'], swmag_plot_df['swmag_auc'], 4)\n",
    "swmag_p = np.poly1d(swmag_z)\n",
    "maxpool_z = np.polyfit(maxpool_plot_df['mean_lat'], maxpool_plot_df['maxpool_auc'], 4)\n",
    "maxpool_p = np.poly1d(maxpool_z)\n",
    "\n",
    "axes.plot(swmag_plot_df['mean_lat'], swmag_p(swmag_plot_df['mean_lat']), color='blue')\n",
    "axes.plot(maxpool_plot_df['mean_lat'], maxpool_p(maxpool_plot_df['mean_lat']), color='orange')\n",
    "\n",
    "axes.set_xlabel('Mean Region MLAT')\n",
    "axes.set_ylabel('PR-AUC Score')\n",
    "axes.set_title('PR-AUC vs Region MLAT')\n",
    "axes.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/pr-auc_vs_region_mlat_swmag_maxpool.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_continuious_reliability_diagram(results, model):\n",
    "\n",
    "\t''' Function that plots the reliability diagram for the predictions.'''\n",
    "\n",
    "\tx = np.linspace(0, 1, 1000)\n",
    "\n",
    "\tfig, ax = plt.subplots(ncols=1, nrows=2, sharex=True, figsize=(10,15))\n",
    "\n",
    "\tfor region in results.keys():\n",
    "\t\tpredictions = results[region][f'{model}_results'].dropna(inplace=False, subset=['actual', 'predicted_mean', 'predicted_std'])\n",
    "\t\tactual = predictions['actual']\n",
    "\t\tpredicted_mean = predictions['predicted_mean']\n",
    "\t\tpredicted_std = predictions['predicted_std'].abs()\n",
    "\n",
    "\t\tstandard_error = (actual - predicted_mean)/(np.sqrt(2) * predicted_std).to_numpy() #Standard error for each parameter\n",
    "\t\tcumulative_dist = np.zeros((len(x), 1)) #Cumulative distribution for each parameter\n",
    "\t\tfor i in standard_error.index:\n",
    "\t\t\tcumulative_dist[:,0] += (1/len(standard_error)) * np.heaviside(x - 0.5*(erf(standard_error.loc[i])+1) , 1) #Calculate the cumulative distribution for each parameter\n",
    "\n",
    "\t\tax[0].plot(x, cumulative_dist[:,0], label=region)\n",
    "\t\tax[1].plot(x, x - cumulative_dist[:,0], label=region)\n",
    "\n",
    "\t#Place legend to the right middle of the figure\n",
    "\tax[0].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.)\n",
    "\tax[0].plot(x, x, linestyle = '--', color = 'k')\n",
    "\tax[0].set_ylabel('Observed Frequency')\n",
    "\tax[0].set_xlim(0,1)\n",
    "\tax[0].set_ylim(0,1)\n",
    "\tfig.suptitle(f'{model} Reliability Diagram')\n",
    "\n",
    "\tax[1].plot(x, np.zeros(len(x)), linestyle = '--', color = 'k')\n",
    "\tax[1].set_ylim(-0.2,0.2)\n",
    "\tax[1].set_xlabel('Predicted Frequency')\n",
    "\tax[1].set_ylabel('Under/Over-\\nEstimation')\n",
    "\tax[1].set_aspect('equal')\n",
    "\tplt.subplots_adjust(hspace = -0.20)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'plots/{model}_reliability_diagram.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_continuious_reliability_diagram(results, model='swmag')\n",
    "plotting_continuious_reliability_diagram(results, model='maxpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for cluster in cluster_dict.values():\n",
    "\tfor key, region in cluster['regions'].items():\n",
    "\t\tif os.path.exists(f'{scaler_dir}/{model_type}_{key}_{VERSION}.pkl'):\n",
    "\t\t\twith open(f'{scaler_dir}/{model_type}_{key}_{VERSION}.pkl', 'rb') as f:\n",
    "\t\t\t\tscaler_values = pickle.load(f)\n",
    "\t\t\t\tscalers[key] = scaler_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cluster_dict.values():\n",
    "\tfor key, region in cluster['regions'].items():\n",
    "\t\tif os.path.exists(f'{shap_dir}/{model_type}_region_{key}_{VERSION}.pkl'):\n",
    "\t\t\twith open(f'{shap_dir}/{model_type}_region_{key}_{VERSION}.pkl', 'rb') as f:\n",
    "\t\t\t\tshap_values = pickle.load(f)\n",
    "\t\t\t\tcluster['regions'][key]['shap_dict'] = shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual_cluster_to_examine = 'canadian_cluster'\n",
    "mean_shap, std_shap, X, mu, sigma, error = [], [], [], [], [], []\n",
    "for cluster in cluster_dict.keys():\n",
    "\tfor reg, region in cluster_dict[cluster]['regions'].items():\n",
    "\t\tif 'shap_dict' in region.keys():\n",
    "\n",
    "\t\t\tregion['shap_dict']['mean_shap'] = np.concatenate([region['shap_dict']['shap_values'][i][0][:,:,:,:] for i in range(len(region['shap_dict']['shap_values']))], axis=0)\n",
    "\t\t\tregion['shap_dict']['std_shap'] = np.concatenate([region['shap_dict']['shap_values'][i][1][:,:,:,:] for i in range(len(region['shap_dict']['shap_values']))], axis=0)\n",
    "\n",
    "\t\t\tfor key in region['shap_dict'].keys():\n",
    "\t\t\t\tif isinstance(region['shap_dict'][key], torch.Tensor):\n",
    "\t\t\t\t\tregion['shap_dict'][key] = region['shap_dict'][key].cpu().detach().numpy()\n",
    "\n",
    "\t\t\tregion['shap_dict']['mean_shap'] = region['shap_dict']['mean_shap'].reshape(region['shap_dict']['mean_shap'].shape[0], region['shap_dict']['mean_shap'].shape[2], region['shap_dict']['mean_shap'].shape[3])\n",
    "\t\t\tregion['shap_dict']['std_shap'] = region['shap_dict']['std_shap'].reshape(region['shap_dict']['std_shap'].shape[0], region['shap_dict']['std_shap'].shape[2], region['shap_dict']['std_shap'].shape[3])\n",
    "\t\t\tregion['shap_dict']['xtest'] = region['shap_dict']['xtest'].reshape(region['shap_dict']['xtest'].shape[0], region['shap_dict']['xtest'].shape[2], region['shap_dict']['xtest'].shape[3])\n",
    "\n",
    "\t\t\tmean_added = np.sum(np.sum(np.abs(region['shap_dict']['mean_shap']), axis=1),axis=1)\n",
    "\t\t\tregion['shap_dict']['mean_shap'] = region['shap_dict']['mean_shap']/mean_added[:,None,None]\n",
    "\n",
    "\t\t\tstd_added = np.sum(np.sum(np.abs(region['shap_dict']['std_shap']), axis=1),axis=1)\n",
    "\t\t\tregion['shap_dict']['std_shap'] = region['shap_dict']['std_shap']/std_added[:,None,None]\n",
    "\n",
    "\t\t\tregion_scaler = scalers[reg]['swmag_scaler']\n",
    "\n",
    "\t\t\tunbroadcasted_mu = results[reg]['swmag_results']['predicted_mean'].to_numpy()\n",
    "\t\t\tunbroadcasted_sigma = results[reg]['swmag_results']['predicted_std'].to_numpy()\n",
    "\t\t\tunbroadcasted_error = np.array(np.abs(results[reg]['swmag_results']['predicted_mean'] - results[reg]['swmag_results']['actual']))\n",
    "\n",
    "\t\t\tconcatenated_mu = np.concatenate([np.concatenate([unbroadcasted_mu[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\t\t\tconcatenated_sigma = np.concatenate([np.concatenate([unbroadcasted_sigma[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\t\t\tconcatenated_error = np.concatenate([np.concatenate([unbroadcasted_error[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\n",
    "\t\t\tmu.append(np.concatenate(concatenated_mu, axis=0))\n",
    "\t\t\tsigma.append(np.concatenate(concatenated_sigma, axis=0))\n",
    "\t\t\terror.append(np.concatenate(concatenated_error, axis=0))\n",
    "\n",
    "\t\t\tmean_shap.append(np.concatenate(region['shap_dict']['mean_shap'], axis=0))\n",
    "\t\t\tstd_shap.append(np.concatenate(region['shap_dict']['std_shap'], axis=0))\n",
    "\t\t\tX.append(region_scaler.inverse_transform(np.concatenate(region['shap_dict']['xtest'], axis=0)))\n",
    "\n",
    "mean_shap = np.concatenate(mean_shap, axis=0)\n",
    "std_shap = np.concatenate(std_shap, axis=0)\n",
    "X = np.concatenate(X, axis=0)\n",
    "mu = np.concatenate(mu, axis=0)\n",
    "sigma = np.concatenate(sigma, axis=0)\n",
    "error = np.concatenate(error, axis=0)\n",
    "\n",
    "mean_shap_df = pd.DataFrame(mean_shap, columns=shap_values['features'])\n",
    "std_shap_df = pd.DataFrame(std_shap, columns=shap_values['features'])\n",
    "transformed_X_df = pd.DataFrame(X, columns=shap_values['features'])\n",
    "mu_df = pd.DataFrame(mu, columns=shap_values['features'])\n",
    "sigma_df = pd.DataFrame(sigma, columns=shap_values['features'])\n",
    "error_df = pd.DataFrame(error, columns=shap_values['features'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['Vx']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(feature), figsize=(15, 10))\n",
    "for i in range(len(feature)):\n",
    "\taxes.hist2d(sigma_df[feature[i]], error_df[feature[i]], bins=100, cmap='viridis', norm=mpl.colors.LogNorm())\n",
    "\taxes.set_title('sigma v error')\n",
    "\taxes.set_xlabel('sigma')\n",
    "\taxes.set_ylabel('error')\n",
    "plt.savefig('plots/swmag_sigma_vs_error.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['dbht_median', 'MAGNITUDE_median']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(feature), figsize=(15, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "\taxes[i].hist2d(transformed_X_df[feature[i]], mean_shap_df[feature[i]], bins=100, cmap='viridis', norm=mpl.colors.LogNorm())\n",
    "\tax.set_title(f'{feature[i]}')\n",
    "\tax.set_xlabel(feature[i])\n",
    "\tax.set_ylabel('SHAP value')\n",
    "plt.savefig(f'plots/{feature[0]}_and_{feature[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'cosMLT'\n",
    "\n",
    "vmin = np.min([np.min(std_shap_df[feature]), np.min(mean_shap_df[feature])])\n",
    "vmax = np.max([np.max(std_shap_df[feature]),np.max(mean_shap_df[feature])])\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(\"AB;DD\", figsize=(15,10))\n",
    "axes['A'].hist2d(transformed_X_df[feature], mean_shap_df[feature], bins=100, cmap='viridis', norm=mpl.colors.LogNorm())\n",
    "axes['B'].hist2d(transformed_X_df[feature], std_shap_df[feature], bins=100, cmap='viridis', norm=mpl.colors.LogNorm())\n",
    "plt.title(f'{feature}')\n",
    "axes['A'].set_ylabel('Percent Contribution')\n",
    "axes['A'].set_xlabel(feature)\n",
    "axes['A'].set_xlim(np.min(transformed_X_df[feature]), np.max(transformed_X_df[feature]))\n",
    "axes['A'].set_ylim(vmin, vmax)\n",
    "axes['A'].set_title('Mean SHAP')\n",
    "\n",
    "axes['B'].set_xlim(np.min(transformed_X_df[feature]), np.max(transformed_X_df[feature]))\n",
    "axes['B'].set_ylim(vmin, vmax)\n",
    "axes['B'].set_xlabel(feature)\n",
    "axes['B'].set_ylabel('Percent Contribution')\n",
    "axes['B'].set_title('STD SHAP')\n",
    "\n",
    "axes['D'].set_title('Comparison')\n",
    "axes['D'].hist(mean_shap_df[feature], bins=100, log=True, histtype='step', label='mean')\n",
    "axes['D'].hist(std_shap_df[feature], bins=100, log=True, histtype='step', label='std')\n",
    "axes['D'].set_ylabel('log count')\n",
    "axes['D'].set_xlabel('Percent Contribution')\n",
    "axes['D'].axvline(0, linestyle='--', color='k')\n",
    "axes['D'].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/{feature}_swmag_shap_value_distributions.png')\n",
    "\n",
    "# getting area under the curve of the 1D histograms\n",
    "from sklearn.metrics import auc\n",
    "mean_hist, mean_bins = np.histogram(mean_shap_df[feature], bins=100)\n",
    "std_hist, std_bins = np.histogram(std_shap_df[feature], bins=100)\n",
    "mean_area = auc(mean_bins[:-1], mean_hist)\n",
    "std_area = auc(std_bins[:-1], std_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('tab20')\n",
    "\n",
    "fig, axes = plt.subplots(2,1, figsize=(15,10))\n",
    "\n",
    "for i, col in enumerate(shap_values['features']):\n",
    "\taxes[0].hist(mean_shap_df[col], bins=100, log=True, histtype='step', label=col, color=colors[i])\n",
    "\taxes[1].hist(std_shap_df[col], bins=100, log=True, histtype='step', label=col, color=colors[i])\n",
    "\n",
    "axes[0].set_title('Mean SHAP')\n",
    "axes[1].set_title('STD SHAP')\n",
    "\n",
    "axes[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_importance_df = mean_shap_df.abs()\n",
    "std_importance_df = std_shap_df.abs()\n",
    "\n",
    "mean_mean_df = mean_importance_df.mean(axis=0)\n",
    "std_mean_df = std_importance_df.mean(axis=0)\n",
    "\n",
    "mean_median_df = mean_importance_df.median(axis=0)\n",
    "std_median_df = std_importance_df.median(axis=0)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(20,10))\n",
    "x = [(i+1)*4 for i in range(len(mean_importance_df.columns))]\n",
    "x_mean = [j-0.5 for j in x]\n",
    "x_std = [k+0.5 for k in x]\n",
    "\n",
    "axes[0].set_title('Importance Mean')\n",
    "axes[0].bar(x_mean, mean_mean_df.values, label='mean')\n",
    "axes[0].bar(x_std, std_mean_df.values, label='std')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(mean_importance_df.columns.tolist(), rotation=75)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('Importance Median')\n",
    "axes[1].bar(x_mean, mean_median_df.values, label='mean')\n",
    "axes[1].bar(x_std, std_median_df.values, label='std')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(mean_importance_df.columns.tolist(), rotation=75)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/swmag_feature_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual_cluster_to_examine = 'canadian_cluster'\n",
    "mean_shap, std_shap, X, mu, sigma, error = [], [], [], [], [], []\n",
    "dataframes_to_make = ['mean_shap_df', 'std_shap_df', 'transformed_X_df', 'mu_df', 'sigma_df', 'error_df']\n",
    "time_dict = {f'{feature}':{} for feature in shap_values['features']}\n",
    "\n",
    "for feat in time_dict.keys():\n",
    "\tfor df in dataframes_to_make:\n",
    "\t\ttime_dict[feat][df] = pd.DataFrame()\n",
    "\n",
    "for cluster in cluster_dict.keys():\n",
    "\tfor reg, region in cluster_dict[cluster]['regions'].items():\n",
    "\t\tif 'shap_dict' in region.keys():\n",
    "\n",
    "\t\t\tregion['shap_dict']['mean_shap'] = np.concatenate([region['shap_dict']['shap_values'][i][0][:,:,:,:] for i in range(len(region['shap_dict']['shap_values']))], axis=0)\n",
    "\t\t\tregion['shap_dict']['std_shap'] = np.concatenate([region['shap_dict']['shap_values'][i][1][:,:,:,:] for i in range(len(region['shap_dict']['shap_values']))], axis=0)\n",
    "\n",
    "\t\t\tfor key in region['shap_dict'].keys():\n",
    "\t\t\t\tif isinstance(region['shap_dict'][key], torch.Tensor):\n",
    "\t\t\t\t\tregion['shap_dict'][key] = region['shap_dict'][key].cpu().detach().numpy()\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tregion['shap_dict']['mean_shap'] = region['shap_dict']['mean_shap'].reshape(region['shap_dict']['mean_shap'].shape[0], region['shap_dict']['mean_shap'].shape[2], region['shap_dict']['mean_shap'].shape[3])\n",
    "\t\t\t\tregion['shap_dict']['std_shap'] = region['shap_dict']['std_shap'].reshape(region['shap_dict']['std_shap'].shape[0], region['shap_dict']['std_shap'].shape[2], region['shap_dict']['std_shap'].shape[3])\n",
    "\t\t\t\tregion['shap_dict']['xtest'] = region['shap_dict']['xtest'].reshape(region['shap_dict']['xtest'].shape[0], region['shap_dict']['xtest'].shape[2], region['shap_dict']['xtest'].shape[3])\n",
    "\n",
    "\t\t\t\tmean_added = np.sum(np.sum(np.abs(region['shap_dict']['mean_shap']), axis=1),axis=1)\n",
    "\t\t\t\tregion['shap_dict']['mean_shap'] = region['shap_dict']['mean_shap']/mean_added[:,None,None]\n",
    "\n",
    "\t\t\t\tstd_added = np.sum(np.sum(np.abs(region['shap_dict']['std_shap']), axis=1),axis=1)\n",
    "\t\t\t\tregion['shap_dict']['std_shap'] = region['shap_dict']['std_shap']/std_added[:,None,None]\n",
    "\n",
    "\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint('We already did this, skipping....')\n",
    "\n",
    "\t\t\tregion_scaler = scalers[reg]['swmag_scaler']\n",
    "\n",
    "\t\t\tunbroadcasted_mu = results[reg]['swmag_results']['predicted_mean'].to_numpy()\n",
    "\t\t\tunbroadcasted_sigma = results[reg]['swmag_results']['predicted_std'].to_numpy()\n",
    "\t\t\tunbroadcasted_error = np.array(np.abs(results[reg]['swmag_results']['predicted_mean'] - results[reg]['swmag_results']['actual']))\n",
    "\n",
    "\t\t\tconcatenated_mu = np.concatenate([np.concatenate([unbroadcasted_mu[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\t\t\tconcatenated_sigma = np.concatenate([np.concatenate([unbroadcasted_sigma[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\t\t\tconcatenated_error = np.concatenate([np.concatenate([unbroadcasted_error[:,np.newaxis]]*30, axis=1)[:,:,np.newaxis]]*14, axis=2)\n",
    "\n",
    "\t\t\tfor v, var in enumerate(time_dict.keys()):\n",
    "\t\t\t\ttime_dict[var]['mean_shap_df'] = pd.concat([time_dict[var]['mean_shap_df'], pd.DataFrame(region['shap_dict']['mean_shap'][:,:,v])], axis=0)\n",
    "\t\t\t\ttime_dict[var]['std_shap_df'] = pd.concat([time_dict[var]['std_shap_df'], pd.DataFrame(region['shap_dict']['std_shap'][:,:,v])], axis=0)\n",
    "\t\t\t\ttime_dict[var]['transformed_X_df'] = pd.concat([time_dict[var]['transformed_X_df'], \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpd.DataFrame(np.array([region_scaler.inverse_transform(region['shap_dict']['xtest'][i,:,:]) \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor i in range(region['shap_dict']['xtest'].shape[0])])[:,:,v])], axis=0)\n",
    "\t\t\t\ttime_dict[var]['mu_df'] = pd.concat([time_dict[var]['mu_df'], pd.DataFrame(concatenated_mu[:,:,v])], axis=0)\n",
    "\t\t\t\ttime_dict[var]['sigma_df'] = pd.concat([time_dict[var]['sigma_df'], pd.DataFrame(concatenated_sigma[:,:,v])], axis=0)\n",
    "\t\t\t\ttime_dict[var]['error_df'] = pd.concat([time_dict[var]['error_df'], pd.DataFrame(concatenated_error[:,:,v])], axis=0)\n",
    "\n",
    "\t\t\tprint(len(time_dict['Vx']['mean_shap_df'].columns))\n",
    "\n",
    "\t\t\t# mu.append(np.concatenate(concatenated_mu, axis=0))\n",
    "\t\t\t# sigma.append(np.concatenate(concatenated_sigma, axis=0))\n",
    "\t\t\t# error.append(np.concatenate(concatenated_error, axis=0))\n",
    "\n",
    "\t\t\t# mean_shap.append(np.concatenate(region['shap_dict']['mean_shap'], axis=0))\n",
    "\t\t\t# std_shap.append(np.concatenate(region['shap_dict']['std_shap'], axis=0))\n",
    "\t\t\t# X.append(region_scaler.inverse_transform(np.concatenate(region['shap_dict']['xtest'], axis=0)))\n",
    "\n",
    "time_columns = [f't-{30-i}' for i in range(0,30)]\n",
    "for var in time_dict.keys():\n",
    "\tfor df in time_dict[var]:\n",
    "\t\tprint(time_dict[var][df].shape)\n",
    "\t\tprint(len(time_dict[var][df].columns))\n",
    "\t\ttime_dict[var][df].columns = time_columns\n",
    "# mean_shap = np.concatenate(mean_shap, axis=0)\n",
    "# std_shap = np.concatenate(std_shap, axis=0)\n",
    "# X = np.concatenate(X, axis=0)\n",
    "# mu = np.concatenate(mu, axis=0)\n",
    "# sigma = np.concatenate(sigma, axis=0)\n",
    "# error = np.concatenate(error, axis=0)\n",
    "\n",
    "# mean_shap_df = pd.DataFrame(mean_shap, columns=shap_values['features'])\n",
    "# std_shap_df = pd.DataFrame(std_shap, columns=shap_values['features'])\n",
    "# transformed_X_df = pd.DataFrame(X, columns=shap_values['features'])\n",
    "# mu_df = pd.DataFrame(mu, columns=shap_values['features'])\n",
    "# sigma_df = pd.DataFrame(sigma, columns=shap_values['features'])\n",
    "# error_df = pd.DataFrame(error, columns=shap_values['features'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = 'BY_GSM'\n",
    "df = 'std_shap_df'\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(20,20))\n",
    "for i, var in enumerate(time_dict.keys()):\n",
    "\tk = i %3\n",
    "\tj = i %5\n",
    "\taxes[j,k].bar([i-30 for i in range(0,30)], time_dict[var][df].abs().mean(axis=0))\n",
    "\taxes[j,k].set_title(var)\n",
    "plt.savefig(f'plots/time_history_{df}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cluster_dict.values():\n",
    "\tfor region in cluster['regions'].values():\n",
    "\t\tregion['mean_lat'] = utils.getting_mean_lat(region['stations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
